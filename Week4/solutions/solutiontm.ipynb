{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Text Mining Exercise Notebook (Solutions)\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "In this IPython Notebook, you are presented with the task of classifying\n",
    "Movie Reviews. We will use a public [movie review][pmr] data set. For\n",
    "convenience, the data has already been pre-cached and processed into two\n",
    "files, one for training and the other for testing. The data were pickled\n",
    "by using the following commands (assuming the files were correctly\n",
    "opened):\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "\n",
    "pickle.dump(train_data, open(path + 'train_m.p', 'wb'))\n",
    "pickle.dump(test_data, open(path + 'test_m.p', 'wb'))\n",
    "```\n",
    "\n",
    "In this Notebook, you will first read in these data, before conducting\n",
    "basic exploratory data analysis. Next, you will begin text mining these\n",
    "data, before finally building text classifiers and evaluating their\n",
    "performance.\n",
    "\n",
    "-----\n",
    "[pmr]: http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "First, you need to read the data into the notebook. Specifically, you\n",
    "want to read (or more correctly _unpickle_)the training data into a\n",
    "variable called `train` and the testing data into a variable called\n",
    "`test`. Next, explore the data, and answer the following questions:\n",
    "\n",
    "1. What are data structures that hold the _training_ and _testing_ data?\n",
    "\n",
    "3. How can you access the _data_ and _labels_ from the _training_ and\n",
    "_testing_ data?\n",
    "\n",
    "2. How many entries are there in the _training_ and _testing_ data?\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unpickle the training and testing data.\n",
    "import pickle\n",
    "\n",
    "with open('/home/data_scientist/data/train_m.p', 'rb') as fin:\n",
    "    train = pickle.load(fin)\n",
    "\n",
    "with open('/home/data_scientist/data/test_m.p', 'rb') as fin:\n",
    "    test = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type of training samples = <class 'dict'>\n",
      "Data Type of testing samples  = <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Display the type of the training and testing data\n",
    "\n",
    "print(\"Data Type of training samples = {0}\".format(type(train)))\n",
    "print(\"Data Type of testing samples  = {0}\".format(type(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data keys = dict_keys(['target', 'data'])\n",
      "Testing data keys = dict_keys(['target', 'data'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data keys = {0}\".format(train.keys()))\n",
    "print(\"Testing data keys = {0}\".format(test.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples = 1500\n",
      "Number of testing samples  = 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training samples = {0}\".format(len(train['data'])))\n",
    "print(\"Number of testing samples  = {0}\".format(len(test['data'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Now that you have the data loaded into the Notebook, and you can access\n",
    "the data and labels, the next step is to tokenize the textual data. For\n",
    "this you can use the `CountVectorizer`. In the second task, you should\n",
    "properly create a `CountVectorizer` that uses English _stop words_. You\n",
    "also should use this text tokenizer to **fit** and **transform** the\n",
    "_training_ data, and to **transform** the _testing_ data so that we can\n",
    "apply and evaluate a classifier in the third step. After these steps are\n",
    "completed, explore the vectorizer to answer these questions:\n",
    "\n",
    "1. How many tokens were produced?\n",
    "\n",
    "1. Is _reboot_ in the list of word tokens?\n",
    "\n",
    "1. Is _fantasy_ in the list of word tokens?\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a CountVectorizer to tokenize the reviews\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english')\n",
    "train_counts = cv.fit_transform(train['data'])\n",
    "test_data = cv.transform(test['data'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary = 35015\n"
     ]
    }
   ],
   "source": [
    "# Display number of words in vocabulary\n",
    "\n",
    "words = cv.vocabulary_\n",
    "\n",
    "print(\"Number of words in vocabulary = {0}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reboot is NOT in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "# Test if 'reboot' is in the vocabulary\n",
    "\n",
    "my_word = u'reboot'\n",
    "\n",
    "if my_word in words:\n",
    "    print(\"{0} is in the vocabulary\".format(my_word))\n",
    "else:\n",
    "    print(\"{0} is NOT in the vocabulary\".format(my_word))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fantasy is in the vocbulary\n"
     ]
    }
   ],
   "source": [
    "# Test if 'fantasy' is in the vocabulary\n",
    "\n",
    "my_word = u'fantasy'\n",
    "\n",
    "if my_word in words:\n",
    "    print(\"{0} is in the vocbulary\".format(my_word))\n",
    "else:\n",
    "    print(\"{0} is NOT in the vocbulary\".format(my_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Now, given a vocabulary, we can first train a classifier, and second,\n",
    "apply the classifier to our test data. Your third task is to train a\n",
    "simple Naive Bayes classifier on the training data and apply this\n",
    "classifier to the test data. After these steps are completed, explore\n",
    "the vectorizer to answer these questions:\n",
    "\n",
    "1. What is your accuracy (compute this by using the `score` method to\n",
    "compare the predicted and actual labels for the test data.)?\n",
    "\n",
    "2. If you limit the vocabulary to 5,000 words, what is your\n",
    "classification accuracy?\n",
    "\n",
    "3. If you don't limit the vocabulary, but do include bigrams, what is\n",
    "your classification accuracy?\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB prediction accuracy =  79.6%\n"
     ]
    }
   ],
   "source": [
    "# Perform simple Naive Bayesian classification, and display prediction accuracy.\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(train_counts, train['target'])\n",
    "\n",
    "predicted = clf.predict(test_data)\n",
    "\n",
    "print(\"NB prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test_data, test['target'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB (MF) prediction accuracy =  80.8%\n"
     ]
    }
   ],
   "source": [
    "# Perform simple Naive Bayesian classificaiton limiting features to 5,000 words, \n",
    "# and display prediction accuracy.\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english', max_features=5000)\n",
    "train_counts = cv.fit_transform(train['data'])\n",
    "test_data = cv.transform(test['data'])\n",
    "\n",
    "clf = MultinomialNB().fit(train_counts, train['target'])\n",
    "\n",
    "predicted = clf.predict(test_data)\n",
    "\n",
    "print(\"NB (MF) prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test_data, test['target'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB (BiGrams) prediction accuracy =  81.4%\n"
     ]
    }
   ],
   "source": [
    "# Perform simple Naive Bayesian classificaiton, including bi-grams, \n",
    "# and display prediction accuracy.\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english', ngram_range=(1,2))\n",
    "train_counts = cv.fit_transform(train['data'])\n",
    "test_data = cv.transform(test['data'])\n",
    "\n",
    "clf = MultinomialNB().fit(train_counts, train['target'])\n",
    "\n",
    "predicted = clf.predict(test_data)\n",
    "\n",
    "print(\"NB (BiGrams) prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test_data, test['target'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "One problem with simply counting word occurrences is that possibly\n",
    "important words can be down-weighted since they might appear\n",
    "infrequently. The standard approach to compensating for this is to\n",
    "employ TF-IDF. Your next task is to integrate TF-IDF into your\n",
    "classification process. First, you need to construct a vocabulary that\n",
    "consists of words and bigrams with a limit of 10,000 for the maximum\n",
    "number of features. Next create a `TfidfTransformer` to fit and\n",
    "transform this new TF-IDF restricted vocabulary. Finally, apply a Naive\n",
    "Bayes classifier.\n",
    "\n",
    "After these steps are completed, explore the answer to these questions:\n",
    "\n",
    "1. What is your classification accuracy? Why is it different?\n",
    "\n",
    "2. If you change the classifier to `LogisticRegression`, what is your\n",
    "classification accuracy? Why did the result change?\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB TF-IDF prediction accuracy =  79.0%\n"
     ]
    }
   ],
   "source": [
    "# Use TF-IDF to classify based on frequencies, not occurances\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english', ngram_range=(1,2), max_features=10000)\n",
    "train_counts = cv.fit_transform(train['data'])\n",
    "test_data = cv.transform(test['data'])\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "train_tfidf = tfidf_transformer.fit_transform(train_counts)\n",
    "\n",
    "clf = MultinomialNB().fit(train_tfidf, train['target'])\n",
    "\n",
    "predicted = clf.predict(tfidf_transformer.transform(test_data))\n",
    "\n",
    "print(\"NB TF-IDF prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test_data, test['target'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR TF-IDF prediction accuracy =  83.8%\n"
     ]
    }
   ],
   "source": [
    "# Use a Logistic Regression model to classify the TF-IDF data.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C=1000).fit(train_tfidf, train['target'])\n",
    "\n",
    "predicted = clf.predict(test_data)\n",
    "print(\"LR TF-IDF prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test_data, test['target'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "\n",
    "In the course Notebooks, we display the confusion matrix as a heat map.\n",
    "In this final task, first modify the course code to display the\n",
    "confusion matrix for this particular text data mining challenge. You\n",
    "should also change the figure size appropriately.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Course function, with two minor modifications (one of which is figsize)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"white\", context='paper', font='monospace')\n",
    "\n",
    "# Convenience function to plot confusion matrix\n",
    "\n",
    "# This method produces a colored heatmap that displays the relationship\n",
    "# between predicted and actual types from a machine leanring method.\n",
    "\n",
    "def confusion(test, predict, title):\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "    # Make a 2D histogram from the test and result arrays\n",
    "    pts, xe, ye = np.histogram2d(test, predict, bins=2)\n",
    "\n",
    "    # For simplicity we create a new DataFrame\n",
    "    pd_pts = pd.DataFrame(pts.astype(int))\n",
    "    \n",
    "    # Display heatmap and add decorations\n",
    "    hm = sns.heatmap(pd_pts, annot=True, fmt=\"d\")\n",
    "    hm.axes.set_title(title)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVcAAAEJCAYAAAAgtWSfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA73goan3rkZ4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f20e3df5550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display confusion matrix for final classification example\n",
    "\n",
    "confusion(test['target'], predicted, 'LR Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Return to the [Week 4 Index](index.ipynb).\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
