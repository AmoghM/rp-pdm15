{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Unix Data Exploration\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "One of the easiest ways to explore a data set is often one of the best.\n",
    "Working at the Unix command line allows you to leverage the many tools\n",
    "that are built-in to the Unix operating system, many of which were\n",
    "designed to rapidly process data. In this lesson, we review some of the\n",
    "basic Unix commands that are useful for exploring data at the command\n",
    "prompt. This includes file viewing tools as well as data processing\n",
    "tools.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Before we can use the Unix tools, we need to first identify a data set.\n",
    "If you are working in the JupyterHub Server, the [airline flight][af]\n",
    "data is already pre-cached in the `data` folder. The main directory\n",
    "includes the flight data for the year 2001 and three supplementary data\n",
    "sets: the airports data, the carrier codes, and plane data. In the\n",
    "`data/raw` directory is the flight data for the years 1987-2008.\n",
    "\n",
    "![Docker sdata](images/docker-sdata.png)\n",
    "\n",
    "If you are working on your laptop, you will need to want these data\n",
    "files and store them in the data directory inside your shared folder,\n",
    "for example, `/home/data_scientist/rppdm/data`. To grab these files, you\n",
    "can execute the following Unix commands, either in an IPython Notebook\n",
    "cell (by preceding them with an exclamation character, `!`), by opening\n",
    "a terminal from your Jupyter server, or by executing them inside a\n",
    "Docker container.\n",
    "\n",
    "```console\n",
    "$ mkdir data\n",
    "$ cd data\n",
    "$ wget http://stat-computing.org/dataexpo/2009/2001.csv.bz2\n",
    "$ bzip2 -d 2001.csv.bz2\n",
    "$ wget http://stat-computing.org/dataexpo/2009/airports.csv\n",
    "$ wget http://stat-computing.org/dataexpo/2009/carriers.csv \n",
    "$ wget http://stat-computing.org/dataexpo/2009/plane-data.csv\n",
    "$ ls -la\n",
    "```\n",
    "\n",
    "When complete, you should have the following files in your Docker\n",
    "container:\n",
    "\n",
    "![Docker data](images/docker-data.png)\n",
    "\n",
    "-----\n",
    "[af]: http://stat-computing.org/dataexpo/2009/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Data\n",
    "\n",
    "Once you have data, the next step is to begin exploring it. When working\n",
    "at the Unix command prompt, you can easily view the beginning and end of\n",
    "a data file by using the `head` and `tail` commands, respectively. A\n",
    "useful flag to include with  these commands is `-n` where `n` is the\n",
    "number of lines you want to display. By default, these commands\n",
    "generally display ten lines, which may be OK or not, depending on the\n",
    "number of columns in the data set. When looking at the beginning of a\n",
    "data set, for example a _CSV_ file, one often wants to look at the first\n",
    "two rows. This is because these types of files often have a header row\n",
    "that lists the column names as the first row. Likewise, it is often a\n",
    "good idea to look at the last few lines of a file to ensure there was no\n",
    "corruption in the data transport process.\n",
    "\n",
    "```console\n",
    "$ head -2 2001.csv\n",
    "$ head airports.csv\n",
    "$ tail -2 2001.csv\n",
    "$ tail -5 carriers.csv\n",
    "```\n",
    "\n",
    "![Docker viewing data](images/docker-view.png)\n",
    "\n",
    "You also can scroll through the contents of a file, which can be useful\n",
    "to quickly get a sense of the types stored in each column as well as the\n",
    "general range of each column. The two Unix commands that are most\n",
    "relevant for this include:\n",
    "\n",
    "- `cat`: which displays the contents of a file continuously to the screen\n",
    "- 'less`: which allows a paginated view of a file's contents (see also\n",
    "the `more` command).\n",
    "\n",
    "For example, the following screen shows the contents of the 2001.csv\n",
    "file.\n",
    "\n",
    "![Docker less view](images/docker-less.png)\n",
    "\n",
    "In this screenshot, the contents of the first few rows show the presence\n",
    "of add characters, in this particular case the characters `<E4><E6>`,\n",
    "which indicate that this data file likely contains characters in a\n",
    "special encoding. In this case, the encoding, which can be found by\n",
    "digging around on the source data site, is _Latin-1_, which we will need\n",
    "to use whenever we read raw data from the `2001.csv` file (or any other\n",
    "year's flight data).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Data Processing\n",
    "\n",
    "In some cases, we simply want to know the number of lines (or rows) in a\n",
    "data file. The `wc` command can be used to obtain this count. By\n",
    "default, `wc` displays multiple values. If you simply want the number of\n",
    "lines you need to use the `-l` flag. For example, to count the number of\n",
    "lines in the `2001.csv` file:\n",
    "\n",
    "```console\n",
    "$ wc -l 2001.csv\n",
    "5967781 2001.csv\n",
    "```\n",
    "\n",
    "The output indicates that there are 5,967,781 rows in this file. Since\n",
    "the first line was a header row, the file contains information on\n",
    "5,967,780 flights for the year 2001. \n",
    "\n",
    "We can use `wc` in combination with the `grep` command to quickly find\n",
    "the number of rows that might contain a particular set of characters.\n",
    "For example, we can find the number of flights in or out of O'hare by\n",
    "using `grep` to find rows that contain `ORD` (the airport code for\n",
    "O'Hare), and use a Unix pipe to connect the output of this command into\n",
    "the input of the `wc command:\n",
    "\n",
    "```console\n",
    "$ grep ORD 2001.csv | wc -l\n",
    "682636\n",
    "```\n",
    "\n",
    "Thus, we should expect that O'Hare handled 682, 636 flights in 2001. We\n",
    "can extend this by connecting multiple `grep` commands together to find\n",
    "out how many flights were between O'Hare and Willard airport in Savoy.\n",
    "\n",
    "```console\n",
    "$ grep ORD 2001.csv | grep CMI | wc -l\n",
    "3652\n",
    "```\n",
    "\n",
    "Of particular interest is how fast these commands operate, which we can\n",
    "see by executing them in an Ipython Notebook cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3652\n",
      "CPU times: user 10 ms, sys: 10 ms, total: 20 ms\n",
      "Wall time: 3.45 s\n"
     ]
    }
   ],
   "source": [
    "%time !grep ORD /home/data_scientist/rppdm/data/2001.csv | grep CMI | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazingly, these simple Unix commands counted the number of flights\n",
    "between these two airports in less than 4 seconds (Wall time refers to\n",
    "what a clock on the wall would report).\n",
    "\n",
    "-----\n",
    "\n",
    "### Basic Data Processing\n",
    "\n",
    "There are a number of Unix commands that can perform basic data\n",
    "processing, such as sorting rows (`sort`), removing duplicate rows\n",
    "(`uniq`), cutting out columns or converting character sequences. To\n",
    "demonstrate, we can use the `cut` command to pull specific columns out\n",
    "of a file. Several flags are useful with the `cut` command. First is the\n",
    "`-d` flag, which is used to indicate the field (or column) separator.\n",
    "For example, to indicate the data are in CSV format, you should use\n",
    "`-d','`. The second useful flag is the `-f` flag, which indicates the\n",
    "fields to extract from each row. With this flag, you can specify a\n",
    "single column, a range of columns, or a set of columns. This is\n",
    "demonstrated in the following code block, where we first extract only\n",
    "column 17, next we extract columns 17 and 19, and finally we extract\n",
    "columns 17 through 19, inclusively.\n",
    "\n",
    "```console\n",
    "$ cut -d',' -f17 2001.csv\n",
    "$ cut -d',' -f17,19,21 2001.csv\n",
    "$ cut -d',' -f17-19 2001.csv\n",
    "```\n",
    "\n",
    "You can try this command out in the following code cell.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin,Dest,Distance\r\n",
      "BWI,CLT,361\r\n",
      "BWI,CLT,361\r\n",
      "BWI,CLT,361\r\n",
      "BWI,CLT,361\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 /home/data_scientist/rppdm/data/2001.csv | cut -d',' -f17-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "#### Character conversion\n",
    "\n",
    "Another useful basic data processing task is character conversion, which\n",
    "can be easily done via the `sed` command. `sed` is an abbreviation for\n",
    "_stream editor_, which exactly describes what this command does, it\n",
    "enables streaming data to be edited or converted on the fly. While this\n",
    "command is extremely powerful, the basic substitution format is simple\n",
    "(substitution is indicated by the `s` character. You specify a _pattern_\n",
    "to match, followed by the _replacement_ text. By default only the first\n",
    "occurrence on each line will be replaced, but you can specify to apply\n",
    "the replacement globally, by appending a `g`. The different  components\n",
    "are combined together as follows: `s/pattern/replacement/g` to indicate\n",
    "that replacement should be substituted for pattern globally in the\n",
    "stream. \n",
    "\n",
    "This command can be used in a Unix pipe or on a file, as demonstrated in\n",
    "the following code block, where the first line flips only the first\n",
    "occurrence of the pattern `ORD`, the second globally replaces all commas\n",
    "with vertical bars, and the final one first replaces commas with spaces\n",
    "before converting the character sequence `NA` to `null`.\n",
    "\n",
    "```console\n",
    "$ sed 's/ORD/DRO/' 2001.csv\n",
    "$ sed 's/,/|/g' 2001.csv\n",
    "$ sed 's/,/ /g' 2001.csv | sed 's/NA/null/g'\n",
    "```\n",
    "\n",
    "The following code blocks demonstrates this command on the airline data\n",
    "set.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001,12,14,5,704,700,1159,1155,DL,678,-N914D,175,175,148,4,4,ONT,DFW,1189,14,13,0,null,0,null,null,null,null,null\n",
      "2001,12,15,6,708,700,1158,1155,DL,678,-N913D,170,175,143,3,8,ONT,DFW,1189,9,18,0,null,0,null,null,null,null,null\n",
      "2001,12,16,7,656,700,1147,1155,DL,678,-N910D,171,175,153,-8,-4,ONT,DFW,1189,7,11,0,null,0,null,null,null,null,null\n",
      "2001,12,17,1,656,700,1151,1155,DL,678,N906D1,175,175,151,-4,-4,ONT,DFW,1189,13,11,0,null,0,null,null,null,null,null\n",
      "2001,12,18,2,709,700,1158,1155,DL,678,N905D1,169,175,143,3,9,ONT,DFW,1189,10,16,0,null,0,null,null,null,null,null\n",
      "CPU times: user 30 ms, sys: 0 ns, total: 30 ms\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%time !sed 's/NA/null/g' /home/data_scientist/rppdm/data/2001.csv | tail -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "This example reinforces the benefits of using Unix command line tools\n",
    "for data processing. In under 15 seconds (on my system), we have\n",
    "converted nearly six million rows of data!\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced data processing\n",
    "\n",
    "So far, the Unix commands we have used were fairly straightforward,\n",
    "generally performing a single task. The next command, `awk`, is tool\n",
    "like a swiss army knife with lots of capabilities. In fact, `awk` is a\n",
    "basic programming language and can perform more complex operations\n",
    "iteratively over every row in a file. The code for an `awk` program can\n",
    "be specified on the command line, or alternatively be saved in a file\n",
    "and executed. The basic approach `awk` employs is to parse a row of data\n",
    "into fields (or columns), which are processed before moving on to the\n",
    "next row. The fields in the current row can be accessed via their column\n",
    "number, which simplifies data processing. In addition, you can specify\n",
    "the delimiter that `awk` should use to parse a line into fields via the\n",
    "`FS=\"\"` flag.\n",
    "\n",
    "As a programming language, `awk` supports conditional statements, which\n",
    "can be used to branch based on the value of one or more fields,\n",
    "functions, including built-ins like `print`, and variables and\n",
    "operators. `awk` also has special `BEGIN` and `END` blocks to perform\n",
    "operations before and after, respectively, parsing the rows in the file.\n",
    "This can be useful for initializing variables or printing the output of\n",
    "calculations that involved all rows. The following code block\n",
    "demonstrate two `awk` examples. First, we use `awk` to print out columns\n",
    "17 and 18 when the departure airport is equal to `ORD`. Second, we\n",
    "compute the average distance of all flights that depart `CMI`.\n",
    "\n",
    "```console\n",
    "$ awk -v FS=\",\" '{if($17 == \"ORD\") print \"Flight Number: \", $10, $17\" to \"$18 ; }' 2001.csv\n",
    "$ awk -v FS=\",\" 'BEGIN{count = 0 ;}{if ($17 == \"ORD\") count += 1 ;} END{print count}' 2001.csv\n",
    "```\n",
    "\n",
    "We demonstrate another example in the following code cell, where we\n",
    "compute the average distance for all flights that depart from Willard\n",
    "airport. Since this statement is so long, we split the command over\n",
    "multiple lines and use the backslash character to indicate that the line\n",
    "continues on to the following line.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average distance =  764.328\n",
      "CPU times: user 20 ms, sys: 10 ms, total: 30 ms\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%time !awk -v FS=\",\" \\\n",
    "'BEGIN{sum >= $19 ; count = 0 ;} \\\n",
    "{if ($17 == \"ORD\") { sum += $19 ; count +=  1 } } \\\n",
    "END{print \"Average distance = \", sum/count}' \\\n",
    "/home/data_scientist/rppdm/data/2001.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "As this example demonstrates, `awk` can be extremely fast, taking less\n",
    "than 11 seconds to compute the average distance of all flights departing\n",
    "from `CMI`.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout Session\n",
    "\n",
    "During this breakout, you should gain experience working with Unix\n",
    "commands presented in this lesson. Specific problems you can attempt\n",
    "include the following:\n",
    "\n",
    "1. Use 'head' top grab out the first 1000 lines from the 2001 Airline\n",
    "data.\n",
    "\n",
    "2. Pipe this data into the `wc` command to verify you have 1000 lines.\n",
    "\n",
    "3. Pipe the frist command into `cut` to extract out the second, fourth,\n",
    "and fifth columns.\n",
    "\n",
    "Additional, more advanced problems:\n",
    "\n",
    "1. Use `head` and `cut` to print out the first, third, and fourth\n",
    "columns from the first 150 rows of the 2001 airline data. \n",
    "\n",
    "2. Use `sed`, `sort`, `awk`, and `uniq` in some order to build a\n",
    "pipeline to print the number of airports located within each state. Note\n",
    "the `-c` flag, which can be used with `uniq`, might be useful.\n",
    "\n",
    "As time permits, return to the code cells in this notebook, make changes\n",
    "and see the results (for example, change airport codes).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Additional References\n",
    "\n",
    "1. The O'Reilly book, [Data Science at the Command Line][dscl]\n",
    "2. [Unix Lessons][ul] from Practical Data Science Short Course\n",
    "\n",
    "-----\n",
    "[dscl]: http://datascienceatthecommandline.com\n",
    "[ul]: https://github.com/ProfessorBrunner/rp-pdss15/blob/master/notebooks/1_unixdp.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return to the [Week One](index.ipynb) index.\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
